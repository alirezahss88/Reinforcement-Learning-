{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e3ef39e",
   "metadata": {},
   "source": [
    "# Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df73ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import time\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b7ec8",
   "metadata": {},
   "source": [
    "# Load the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a788e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e7601",
   "metadata": {},
   "source": [
    "# Writing the taxi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "615c44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Taxi3():\n",
    "    \"\"\"\n",
    "    The class takes the Taxi environmnet created by OpenAI and train the agent using SARSA algorithm.\n",
    "    Args:\n",
    "        env: the predefined Taxi environment\n",
    "        n_epochs: total number of episodes\n",
    "        lr: learning rate, default: 0.01\n",
    "        df: discount factor, default: 0.99\n",
    "        init_epsilon: initial probability of exploration, default 1.\n",
    "        decay_epsilon: the factor by which the epsilon value dereases exponentialy, defalut 0.90.\n",
    "        min_epsilon: the minimum likelihood of exploration, default 0.01.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_epochs, *, lr=0.3, df= 0.99, init_epsilon=1, \n",
    "                 min_epsilon=0.01, decay_epsilon=0.9):\n",
    "        \n",
    "        self.env = env\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.df = df\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.epsilon = self.init_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_epsilon = decay_epsilon\n",
    "        \n",
    "        # the Q table is initialized with random values between -0.5 and +0.5\n",
    "        self.Q = np.random.uniform(-0.5, 0.5, size=(env.observation_space.n, env.action_space.n))\n",
    "        self.trained_ = False\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the model using the predefined hyperparameters.\n",
    "        \"\"\"\n",
    "        for eps in range(self.n_epochs):\n",
    "            \n",
    "            self.epsilon = self.init_epsilon     #at the beginning of each episode, epsilon is reset to its initial value\n",
    "            s = self.env.reset()\n",
    "            a = self.env.action_space.sample()\n",
    "            \n",
    "            for t in range(self.env.spec.max_episode_steps):\n",
    "                s_, r, done, __ = self.env.step(a)               #the env returns the next state, s_, and reward, r. \n",
    "                a_ = np.argmax(self.Q[s_, :])                    #the action is chosen based on the e-greedy policy\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    a_ = self.env.action_space.sample()\n",
    "                    \n",
    "                #if epsilon is bigger than its minimum, it should be multiplied by the decay value\n",
    "                if self.epsilon > self.min_epsilon:       \n",
    "                    self.epsilon *= self.decay_epsilon\n",
    "                \n",
    "                #the Q table is updated based on the action chosen above and the next state returned by the env\n",
    "                self.Q[s, a] += self.lr * (r + self.df * self.Q[s_, a_] - self.Q[s, a])\n",
    "                \n",
    "                #old state and action are replaced by the next state and action obtained above\n",
    "                a = a_\n",
    "                s = s_\n",
    "                \n",
    "                if done:\n",
    "                    print(f'Episode {eps} finished after {t} time steps')\n",
    "                    break\n",
    "        self.trained_ = True\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        run the model for one episode using the trained model.\n",
    "        \"\"\"\n",
    "        if self.trained_:\n",
    "            s = self.env.reset()\n",
    "            a = self.env.action_space.sample()\n",
    "            for t_ in range(self.env.spec.max_episode_steps):\n",
    "                clear_output(wait=True)\n",
    "                s_, r, done, __ = self.env.step(a)\n",
    "                self.env.render()\n",
    "                a_ = np.argmax(self.Q[s_, :])\n",
    "                a = a_\n",
    "                s = s_\n",
    "                time.sleep(0.8)\n",
    "                if done:\n",
    "                    break\n",
    "            print(f'Finished in {t_} time steps')\n",
    "        else:\n",
    "            print('The model is not trained yet. First call the train method.')\n",
    "                \n",
    "    def reset_Q(self):\n",
    "        \"\"\"\n",
    "        Reset the Q table if the method is called. The test method is not available after calling this method.\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.trained_ = False\n",
    "    \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
