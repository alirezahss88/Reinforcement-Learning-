{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a237164",
   "metadata": {},
   "source": [
    "## MountainCar-v0 with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8f536",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, we import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "024533b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fa40d3",
   "metadata": {},
   "source": [
    "Then, we instantiate the environment using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdbcbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca224f31",
   "metadata": {},
   "source": [
    "The problem is continuous state and solving such problems using Q-learning algorithm is impossible due to discrete nature of the TD methods. In order to solve the issue, the problem needs to be broken into discrete states.\n",
    "\n",
    "To do so, first, we obtain the high and low bound of the observation space. Please note that the environment has 2 states at each time step, the position and velocity state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7faf43cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low bound of position state: -1.2000000476837158\n",
      "High bound of position state: 0.6000000238418579\n",
      "Low bound of velocity state: -0.07000000029802322\n",
      "High bound of velocity state: 0.07000000029802322\n"
     ]
    }
   ],
   "source": [
    "print(f'Low bound of position state: {env.observation_space.low[0]}')\n",
    "print(f'High bound of position state: {env.observation_space.high[0]}')\n",
    "print(f'Low bound of velocity state: {env.observation_space.low[1]}')\n",
    "print(f'High bound of velocity state: {env.observation_space.high[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fb096b",
   "metadata": {},
   "source": [
    "The length of the position and velocity state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f924790d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of position state: 1.2700001001358032\n",
      "Length of velocity state: 0.14000000059604645\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of position state: {env.observation_space.high[1] - env.observation_space.low[0]}')\n",
    "print(f'Length of velocity state: {env.observation_space.high[1] - env.observation_space.low[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24deae",
   "metadata": {},
   "source": [
    "We break the position and velocity states into 20 and 200 discrete states, respectively. Thus, at each time step, the observations for the position and velocity should be multiplied by 20 and 200 and converted into integer values. (Note that the chosen values, i.e. 20 and 200, are arbitrary. By increasing the values, the steps would be finer and consequently, the required computational budget would increase. Lower values will decrease the required memory and the training time and will diminish the precision of the model).\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f3780d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discritized position state -9\n",
      "Discritized velocity state 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(f'Discritized position state {int(20*state[0])}')\n",
    "print(f'Discritized velocity state {int(200*state[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e042dc25",
   "metadata": {},
   "source": [
    "As you know, to solve a problem using the Q-learning algorithm we need to build a state-action table that maps every pair of state and action to its corresponding value.\n",
    "\n",
    "Looking at the above values, we find out that the obtained values are negative at some states which results in a wrong answer. So, we need to shift both of the states by a value to make sure that they cannot get a negative value. Those values are the absolute value of the low bound of each state multiplied by their corresponding discretizing factors. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b4fe8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discritized position state 15\n",
      "Discritized velocity state 14\n"
     ]
    }
   ],
   "source": [
    "print(f'Discritized position state {int(20*state[0]) + abs(int(20*env.observation_space.low[0]))}')\n",
    "print(f'Discritized velocity state {int(200*state[1]) + abs(int(200*env.observation_space.low[1]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b578f45a",
   "metadata": {},
   "source": [
    "ALL SET!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934e255d",
   "metadata": {},
   "source": [
    "Now, it is time to specify hyperparameters as follow:\n",
    "\n",
    "$\n",
    "learning\\,rate = 0.4 \\\\\n",
    "discount\\,rate(gamma)= 0.99\\\\\n",
    "initial\\,\\epsilon = 0.5 \\\\\n",
    "\\epsilon\\,decay = 0.90 \\\\\n",
    "minimum\\,\\epsilon = 0.01 \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339442ba",
   "metadata": {},
   "source": [
    "Then, we are ready to write the *MountainCar* class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "dbf81a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCar():\n",
    "    \"\"\"\n",
    "    A class to train and test the MountainCar-v0 created by OpenAI.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_epochs, *, lr=0.4, df=0.99, init_epsilon=0.5, \n",
    "                 min_epsilon = 0.01, decay_epsilon=0.90, dis_factor=20):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            env: Instantiated MountainCar-v0 environment.\n",
    "            n_epochs: Number of epochs to train the model.\n",
    "            lr: Learning rate, default 0.4.\n",
    "            df: discount factor or gamma, default 0.99.\n",
    "            init_epsilon: initial probability of exploration, default 0.5.\n",
    "            decay_epsilon: the factor by which the epsilon value dereases exponentialy, defalut 0.90.\n",
    "            min_epsilon: the minimum likelihood of exploration, default 0.01.\n",
    "            dis_factor: The factor by which the Position state is discretized. This is ten times bigger for the Velocity state. default 20.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.df = df\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_epsilon = decay_epsilon\n",
    "        self.dis_factor = dis_factor\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.upper_position = self.env.observation_space.high[0]     #obtatinig high bound of position state\n",
    "        self.lower_position = self.env.observation_space.low[0]      #obtatinig low bound of position state\n",
    "        self.upper_velocity = self.env.observation_space.high[1]     #obtatinig high bound of velocity state\n",
    "        self.lower_velocity = self.env.observation_space.low[1]      #obtatinig low bound of velocity state\n",
    "        \n",
    "        self.shift_position = np.abs(int(self.lower_position * self.dis_factor))       #shifting the positions \n",
    "                                                                                       #to get positive values\n",
    "        self.shift_velocity = np.abs(int(self.lower_velocity * self.dis_factor * 10))  #shifting the velocities \n",
    "                                                                                       #to get positive values\n",
    "        self.n_state = self.env.observation_space.shape[0]          #number of state types, position and velocity\n",
    "        self.n_action = self.env.action_space.n         #number of possible actions, accelerate, neutral, decelerate\n",
    "\n",
    "        self.ave_reward_list = []              #creating a list to store average reward every 100 episodes\n",
    "        self.reward_list = []                  #creating a list to store 100 total rewards at each episode\n",
    "        \n",
    "        self.n_state_position = int((self.upper_position - self.lower_position) * self.dis_factor)\n",
    "        self.n_state_velocity = int((self.upper_velocity - self.lower_velocity) * self.dis_factor * 10)\n",
    "        #initializing the Q table with random values between -0.5 and +0.5\n",
    "        self.Q = np.random.uniform(-0.5, 0.5, size=(self.n_state_position, self.n_state_velocity, self.n_action))\n",
    "        \n",
    "        self.trained_ = False\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train the model using the predefined hyperparameters.\n",
    "        \"\"\"\n",
    "        for e in range(self.n_epochs):\n",
    "            \n",
    "            tot_reward = 0           #the total reward returned by the env during each episode          \n",
    "            \n",
    "            s = self.env.reset()     #the environment should be reset at the beginning of each episode\n",
    "            self.epsilon = self.init_epsilon    #the probability of exploration should be reset at \n",
    "                                                #the beginning of each episode\n",
    "            \n",
    "            for t in range(200):\n",
    "                \n",
    "                if e % 100 == 0:\n",
    "                    env.render()\n",
    "                    \n",
    "                #obtatining shifted position state, sp, and velocity space, sv.\n",
    "                sp = int(s[0]*self.dis_factor) + self.shift_position\n",
    "                sv = int(s[1]*self.dis_factor*10) + self.shift_velocity\n",
    "                #choosing the next action based on e-greedy policy\n",
    "                a = np.argmax(self.Q[sp, sv, :])\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    a = self.env.action_space.sample()\n",
    "                #decaying the epsilon if it is bigger than minimum epsilon\n",
    "                if self.epsilon > self.min_epsilon:\n",
    "                    self.epsilon *= self.decay_epsilon\n",
    "                #the env step forward and returns next state, s_, reward, r and if the goal is hit\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                sp_ = int(s_[0]*self.dis_factor) + self.shift_position\n",
    "                sv_ = int(s_[1]*self.dis_factor*10) + self.shift_velocity\n",
    "                #updating the Q table using the greedy policy\n",
    "                self.Q[sp, sv, a] += self.lr * (r + self.df * np.max(self.Q[sp_, sv_, :] - self.Q[sp, sv, a]))\n",
    "                \n",
    "                \n",
    "                tot_reward += r\n",
    "        \n",
    "                if done:\n",
    "                    break\n",
    "                #setting the next state as the current state for the next time step\n",
    "                s = s_\n",
    "            self.reward_list.append(tot_reward)\n",
    "            if (e+1) % 100 == 0:\n",
    "                ave_reward = np.mean(self.reward_list)\n",
    "                self.ave_reward_list.append(ave_reward)\n",
    "                self.reward_list = []\n",
    "                print(f'episode {e} finished in {t} time steps and reward is {ave_reward}')\n",
    "        \n",
    "        env.close()\n",
    "        self.trained_ = True\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        run the model for one episode using the trained model.\n",
    "        \"\"\"\n",
    "        if self.trained_:\n",
    "            s = self.env.reset()\n",
    "            for t in range(200):\n",
    "                env.render()\n",
    "                sp = int(s[0]*self.dis_factor) + self.shift_position\n",
    "                sv = int(s[1]*self.dis_factor*10) + self.shift_velocity\n",
    "                a = np.argmax(self.Q[sp, sv, :])\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                s = s_\n",
    "                if done:\n",
    "                    print(f'Finished in {t} time steps.')\n",
    "                    break\n",
    "\n",
    "            env.close()\n",
    "        else:\n",
    "            print('The model is not trained yet. First train the model using the Train method.')\n",
    "    def reset_Q(self):\n",
    "        \"\"\"\n",
    "        Reset the Q table if the method is called. The test method is not available after calling this method.\n",
    "        \"\"\"\n",
    "        self.Q = np.random.uniform(-0.1, 0.1, size=(self.n_state_position, self.n_state_velocity, self.n_action))\n",
    "        self.trained_ = False\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "69de026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountaincar = MountainCar(env, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "dedd8f38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 99 finished in 199 time steps and reward is -200.0\n",
      "episode 199 finished in 199 time steps and reward is -200.0\n",
      "episode 299 finished in 199 time steps and reward is -200.0\n",
      "episode 399 finished in 199 time steps and reward is -200.0\n",
      "episode 499 finished in 199 time steps and reward is -200.0\n",
      "episode 599 finished in 199 time steps and reward is -199.12\n",
      "episode 699 finished in 199 time steps and reward is -198.92\n",
      "episode 799 finished in 199 time steps and reward is -200.0\n",
      "episode 899 finished in 199 time steps and reward is -200.0\n",
      "episode 999 finished in 199 time steps and reward is -198.59\n",
      "episode 1099 finished in 199 time steps and reward is -199.17\n",
      "episode 1199 finished in 197 time steps and reward is -192.27\n",
      "episode 1299 finished in 199 time steps and reward is -198.97\n",
      "episode 1399 finished in 199 time steps and reward is -200.0\n",
      "episode 1499 finished in 189 time steps and reward is -197.39\n",
      "episode 1599 finished in 199 time steps and reward is -192.25\n",
      "episode 1699 finished in 166 time steps and reward is -191.55\n",
      "episode 1799 finished in 199 time steps and reward is -192.24\n",
      "episode 1899 finished in 199 time steps and reward is -195.01\n",
      "episode 1999 finished in 176 time steps and reward is -193.76\n",
      "episode 2099 finished in 155 time steps and reward is -188.27\n",
      "episode 2199 finished in 199 time steps and reward is -192.3\n",
      "episode 2299 finished in 199 time steps and reward is -195.27\n",
      "episode 2399 finished in 171 time steps and reward is -189.15\n",
      "episode 2499 finished in 199 time steps and reward is -185.47\n",
      "episode 2599 finished in 199 time steps and reward is -193.44\n",
      "episode 2699 finished in 199 time steps and reward is -197.04\n",
      "episode 2799 finished in 199 time steps and reward is -196.8\n",
      "episode 2899 finished in 199 time steps and reward is -189.92\n",
      "episode 2999 finished in 147 time steps and reward is -176.2\n",
      "episode 3099 finished in 156 time steps and reward is -179.92\n",
      "episode 3199 finished in 188 time steps and reward is -176.46\n",
      "episode 3299 finished in 155 time steps and reward is -181.15\n",
      "episode 3399 finished in 155 time steps and reward is -184.04\n",
      "episode 3499 finished in 193 time steps and reward is -186.78\n",
      "episode 3599 finished in 164 time steps and reward is -194.45\n",
      "episode 3699 finished in 199 time steps and reward is -198.01\n",
      "episode 3799 finished in 155 time steps and reward is -189.84\n",
      "episode 3899 finished in 199 time steps and reward is -181.81\n",
      "episode 3999 finished in 199 time steps and reward is -199.06\n",
      "episode 4099 finished in 199 time steps and reward is -196.91\n",
      "episode 4199 finished in 160 time steps and reward is -190.94\n",
      "episode 4299 finished in 199 time steps and reward is -187.34\n",
      "episode 4399 finished in 199 time steps and reward is -197.08\n",
      "episode 4499 finished in 199 time steps and reward is -197.1\n",
      "episode 4599 finished in 199 time steps and reward is -188.56\n",
      "episode 4699 finished in 122 time steps and reward is -176.06\n",
      "episode 4799 finished in 199 time steps and reward is -187.31\n",
      "episode 4899 finished in 199 time steps and reward is -180.91\n",
      "episode 4999 finished in 195 time steps and reward is -182.63\n",
      "episode 5099 finished in 199 time steps and reward is -177.48\n",
      "episode 5199 finished in 199 time steps and reward is -187.86\n",
      "episode 5299 finished in 199 time steps and reward is -188.4\n",
      "episode 5399 finished in 189 time steps and reward is -184.47\n",
      "episode 5499 finished in 187 time steps and reward is -188.76\n",
      "episode 5599 finished in 199 time steps and reward is -192.55\n",
      "episode 5699 finished in 158 time steps and reward is -181.32\n",
      "episode 5799 finished in 152 time steps and reward is -175.67\n",
      "episode 5899 finished in 199 time steps and reward is -183.92\n",
      "episode 5999 finished in 152 time steps and reward is -181.18\n",
      "episode 6099 finished in 199 time steps and reward is -180.09\n",
      "episode 6199 finished in 199 time steps and reward is -174.51\n",
      "episode 6299 finished in 165 time steps and reward is -171.67\n",
      "episode 6399 finished in 129 time steps and reward is -166.68\n",
      "episode 6499 finished in 171 time steps and reward is -165.01\n",
      "episode 6599 finished in 132 time steps and reward is -162.64\n",
      "episode 6699 finished in 129 time steps and reward is -159.84\n",
      "episode 6799 finished in 199 time steps and reward is -162.2\n",
      "episode 6899 finished in 150 time steps and reward is -182.33\n",
      "episode 6999 finished in 127 time steps and reward is -155.31\n",
      "episode 7099 finished in 120 time steps and reward is -154.72\n",
      "episode 7199 finished in 160 time steps and reward is -161.39\n",
      "episode 7299 finished in 144 time steps and reward is -155.97\n",
      "episode 7399 finished in 130 time steps and reward is -171.08\n",
      "episode 7499 finished in 199 time steps and reward is -185.83\n",
      "episode 7599 finished in 199 time steps and reward is -191.68\n",
      "episode 7699 finished in 123 time steps and reward is -182.25\n",
      "episode 7799 finished in 122 time steps and reward is -190.55\n",
      "episode 7899 finished in 154 time steps and reward is -173.17\n",
      "episode 7999 finished in 154 time steps and reward is -181.16\n",
      "episode 8099 finished in 160 time steps and reward is -181.85\n",
      "episode 8199 finished in 196 time steps and reward is -175.18\n",
      "episode 8299 finished in 159 time steps and reward is -182.33\n",
      "episode 8399 finished in 151 time steps and reward is -176.22\n",
      "episode 8499 finished in 159 time steps and reward is -173.94\n",
      "episode 8599 finished in 149 time steps and reward is -170.41\n",
      "episode 8699 finished in 149 time steps and reward is -166.63\n",
      "episode 8799 finished in 148 time steps and reward is -172.91\n",
      "episode 8899 finished in 121 time steps and reward is -161.36\n",
      "episode 8999 finished in 175 time steps and reward is -162.07\n",
      "episode 9099 finished in 166 time steps and reward is -163.39\n",
      "episode 9199 finished in 148 time steps and reward is -173.66\n",
      "episode 9299 finished in 182 time steps and reward is -162.42\n",
      "episode 9399 finished in 199 time steps and reward is -168.42\n",
      "episode 9499 finished in 167 time steps and reward is -167.51\n",
      "episode 9599 finished in 178 time steps and reward is -176.11\n",
      "episode 9699 finished in 164 time steps and reward is -193.97\n",
      "episode 9799 finished in 183 time steps and reward is -197.45\n",
      "episode 9899 finished in 199 time steps and reward is -197.9\n",
      "episode 9999 finished in 199 time steps and reward is -195.57\n"
     ]
    }
   ],
   "source": [
    "mountaincar.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
