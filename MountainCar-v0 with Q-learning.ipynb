{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a237164",
   "metadata": {},
   "source": [
    "## MountainCar-v0 with Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8f536",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First, we import relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024533b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f01d2",
   "metadata": {},
   "source": [
    "Then, we instantiate the environment using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbcbc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b9de44",
   "metadata": {},
   "source": [
    "The problem is continuous state and solving such problems using Q-learning algorithm is impossible due to discrete nature of the TD methods. In order to solve the issue, the problem needs to be broken into discrete states.\n",
    "\n",
    "To do so, first, we obtain the high and low bound of the observation space. Please note that the environment has 2 states at each time step, the position and velocity state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834d2a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low bound of position state: -1.2000000476837158\n",
      "High bound of position state: 0.6000000238418579\n",
      "Low bound of velocity state: -0.07000000029802322\n",
      "High bound of velocity state: 0.07000000029802322\n"
     ]
    }
   ],
   "source": [
    "print(f'Low bound of position state: {env.observation_space.low[0]}')\n",
    "print(f'High bound of position state: {env.observation_space.high[0]}')\n",
    "print(f'Low bound of velocity state: {env.observation_space.low[1]}')\n",
    "print(f'High bound of velocity state: {env.observation_space.high[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c1162",
   "metadata": {},
   "source": [
    "The length of the position and velocity state:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed10b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of position state: 1.2700001001358032\n",
      "Length of velocity state: 0.14000000059604645\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of position state: {env.observation_space.high[1] - env.observation_space.low[0]}')\n",
    "print(f'Length of velocity state: {env.observation_space.high[1] - env.observation_space.low[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c87957",
   "metadata": {},
   "source": [
    "We break the position and velocity states into 20 and 200 discrete states, respectively. Thus, at each time step, the observations for the position and velocity should be multiplied by 20 and 200 and converted into integer values. (Note that the chosen values, i.e. 20 and 200, are arbitrary. By increasing the values, the steps would be finer and consequently, the required computational budget would increase. Lower values will decrease the required memory and the training time and will diminish the precision of the model).\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d901140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discritized position state -9\n",
      "Discritized velocity state 0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print(f'Discritized position state {int(20*state[0])}')\n",
    "print(f'Discritized velocity state {int(200*state[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68fba52",
   "metadata": {},
   "source": [
    "As you know, to solve a problem using the Q-learning algorithm we need to build a state-action table that maps every pair of state and action to its corresponding value.\n",
    "\n",
    "Looking at the above values, we find out that the obtained values are negative at some states which results in a wrong answer. So, we need to shift both of the states by a value to make sure that they cannot get a negative value. Those values are the absolute value of the low bound of each state multiplied by their corresponding discretizing factors. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53971ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discritized position state 15\n",
      "Discritized velocity state 14\n"
     ]
    }
   ],
   "source": [
    "print(f'Discritized position state {int(20*state[0]) + abs(int(20*env.observation_space.low[0]))}')\n",
    "print(f'Discritized velocity state {int(200*state[1]) + abs(int(200*env.observation_space.low[1]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769a8207",
   "metadata": {},
   "source": [
    "ALL SET!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe23a3",
   "metadata": {},
   "source": [
    "Now, it is time to specify hyperparameters as follow:\n",
    "\n",
    "$\n",
    "learning\\,rate = 0.4 \\\\\n",
    "discount\\,rate(gamma)= 0.99\\\\\n",
    "initial\\,\\epsilon = 0.5 \\\\\n",
    "\\epsilon\\,decay = 0.90 \\\\\n",
    "minimum\\,\\epsilon = 0.01 \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac686381",
   "metadata": {},
   "source": [
    "Then, we are ready to write the *MountainCar* class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf81a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCar():\n",
    "    \"\"\"\n",
    "    A class to train and test the MountainCar-v0 created by OpenAI.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_epochs, *, lr=0.4, df=0.99, init_epsilon=0.5, \n",
    "                 min_epsilon = 0.01, decay_epsilon=0.90, dis_factor=20):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "            env: Instantiated MountainCar-v0 environment.\n",
    "            n_epochs: Number of epochs to train the model.\n",
    "            lr: Learning rate, default 0.4.\n",
    "            df: discount factor or gamma, default 0.99.\n",
    "            init_epsilon: initial probability of exploration, default 0.5.\n",
    "            decay_epsilon: the factor by which the epsilon value dereases exponentialy, defalut 0.90.\n",
    "            min_epsilon: the minimum likelihood of exploration, default 0.01.\n",
    "            dis_factor: The factor by which the Position state is discretized. This is ten times bigger for the Velocity state. default 20.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.df = df\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.decay_epsilon = decay_epsilon\n",
    "        self.dis_factor = dis_factor\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.upper_position = self.env.observation_space.high[0]     #obtatinig high bound of position state\n",
    "        self.lower_position = self.env.observation_space.low[0]      #obtatinig low bound of position state\n",
    "        self.upper_velocity = self.env.observation_space.high[1]     #obtatinig high bound of velocity state\n",
    "        self.lower_velocity = self.env.observation_space.low[1]      #obtatinig low bound of velocity state\n",
    "        \n",
    "        self.shift_position = np.abs(int(self.lower_position * self.dis_factor))       #shifting the positions \n",
    "                                                                                       #to get positive values\n",
    "        self.shift_velocity = np.abs(int(self.lower_velocity * self.dis_factor * 10))  #shifting the velocities \n",
    "                                                                                       #to get positive values\n",
    "        self.n_state = self.env.observation_space.shape[0]          #number of state types, position and velocity\n",
    "        self.n_action = self.env.action_space.n         #number of possible actions, accelerate, neutral, decelerate\n",
    "\n",
    "        self.ave_reward_list = []              #creating a list to store average reward every 100 episodes\n",
    "        self.reward_list = []                  #creating a list to store 100 total rewards at each episode\n",
    "        \n",
    "        self.n_state_position = int((self.upper_position - self.lower_position) * self.dis_factor)\n",
    "        self.n_state_velocity = int((self.upper_velocity - self.lower_velocity) * self.dis_factor * 10)\n",
    "        #initializing the Q table with random values between -0.5 and +0.5\n",
    "        self.Q = np.random.uniform(-0.5, 0.5, size=(self.n_state_position, self.n_state_velocity, self.n_action))\n",
    "        \n",
    "        self.trained_ = False\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train the model using the predefined hyperparameters.\n",
    "        \"\"\"\n",
    "        for e in range(self.n_epochs):\n",
    "            \n",
    "            tot_reward = 0           #the total reward returned by the env during each episode          \n",
    "            \n",
    "            s = self.env.reset()     #the environment should be reset at the beginning of each episode\n",
    "            self.epsilon = self.init_epsilon    #the probability of exploration should be reset at \n",
    "                                                #the beginning of each episode\n",
    "            \n",
    "            for t in range(200):\n",
    "                \n",
    "                if e % 100 == 0:\n",
    "                    env.render()\n",
    "                    \n",
    "                #obtatining shifted position state, sp, and velocity space, sv.\n",
    "                sp = int(s[0]*self.dis_factor) + self.shift_position\n",
    "                sv = int(s[1]*self.dis_factor*10) + self.shift_velocity\n",
    "                #choosing the next action based on e-greedy policy\n",
    "                a = np.argmax(self.Q[sp, sv, :])\n",
    "                if np.random.random() < self.epsilon:\n",
    "                    a = self.env.action_space.sample()\n",
    "                #decaying the epsilon if it is bigger than minimum epsilon\n",
    "                if self.epsilon > self.min_epsilon:\n",
    "                    self.epsilon *= self.decay_epsilon\n",
    "                #the env step forward and returns next state, s_, reward, r and if the goal is hit\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                sp_ = int(s_[0]*self.dis_factor) + self.shift_position\n",
    "                sv_ = int(s_[1]*self.dis_factor*10) + self.shift_velocity\n",
    "                #updating the Q table using the greedy policy\n",
    "                self.Q[sp, sv, a] += self.lr * (r + self.df * np.max(self.Q[sp_, sv_, :] - self.Q[sp, sv, a]))\n",
    "                \n",
    "                \n",
    "                tot_reward += r\n",
    "        \n",
    "                if done:\n",
    "                    break\n",
    "                #setting the next state as the current state for the next time step\n",
    "                s = s_\n",
    "            self.reward_list.append(tot_reward)\n",
    "            if (e+1) % 100 == 0:\n",
    "                ave_reward = np.mean(self.reward_list)\n",
    "                self.ave_reward_list.append(ave_reward)\n",
    "                self.reward_list = []\n",
    "                print(f'episode {e} finished in {t} time steps and reward is {ave_reward}')\n",
    "        \n",
    "        env.close()\n",
    "        self.trained_ = True\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"\n",
    "        run the model for one episode using the trained model.\n",
    "        \"\"\"\n",
    "        if self.trained_:\n",
    "            s = self.env.reset()\n",
    "            for t in range(200):\n",
    "                env.render()\n",
    "                sp = int(s[0]*self.dis_factor) + self.shift_position\n",
    "                sv = int(s[1]*self.dis_factor*10) + self.shift_velocity\n",
    "                a = np.argmax(self.Q[sp, sv, :])\n",
    "                s_, r, done, _ = self.env.step(a)\n",
    "                s = s_\n",
    "                if done:\n",
    "                    print(f'Finished in {t} time steps.')\n",
    "                    break\n",
    "\n",
    "            env.close()\n",
    "        else:\n",
    "            print('The model is not trained yet. First train the model using the Train method.')\n",
    "    def reset_Q(self):\n",
    "        \"\"\"\n",
    "        Reset the Q table if the method is called. The test method is not available after calling this method.\n",
    "        \"\"\"\n",
    "        self.Q = np.random.uniform(-0.1, 0.1, size=(self.n_state_position, self.n_state_velocity, self.n_action))\n",
    "        self.trained_ = False\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
